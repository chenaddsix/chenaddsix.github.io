---
layout: post
title: 贝叶斯方法
date: 2017-11-26
categories: ML
tags: [ML]
description: Bayes Methods
---

<script type="text/javascript" async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

## 一.贝叶斯方法
在贝叶斯之前，人们主要通过频率来确定一件事情的概率，而贝叶斯方法的提出改变了这种情况，贝叶斯理论提供了一种新的想法：$$先验p(\theta)+样本信息x \Rightarrow 后验分布p(\theta|x)$$，其中先验信息一般来源于历史资料，即频率的观点。在得到新的样本信息之后，贝叶斯认为不应仍然依据频率的观点，而是由给定x的情况下$$\theta$$的条件概率决定，而使这个条件概率最大的$$\theta_{MD}$$称为最大后验估计，即极大似然估计。
<br>
$$\omega_i$$是样本$$x$$的类别集合，概率密度函数$$p(x|\omega_i)$$是相对于$$x$$的$$\omega_i$$的似然函数，对于一个分类问题，即求得$$x$$最可能的类别概率$$p(\omega_i|x)$$，由贝叶斯规则有：
<center> $$ P(\omega _i|x) = \frac {p(x|\omega _i)P(\omega _i)} {p(x)} (*)$$ </center>
此时，我们只需求最大的$$P(\omega _i|x)$$就可知道$$x$$的类别。对于多维度特征必定存在一个决策面将每个类别划分，例如下图中的$$x=x_0$$使得$$P(\omega_i|x)-P(\omega_j|x)=0$$
<center> <img src="https://github.com/blogchenjl/MarkdownPic/blob/Razor_Atmel/Bayes_1.PNG?raw=true"  alt=" " /> </center>
若用判别函数$$g(x)=f(P(\omega_i|x))$$表示，其中$$f(\cdot)$$是单调升函数，决策面可以表示为：
<center> $$g_{ij}(x)=g_i(x)-g_j(x)=0, i,j=1,2,\cdots,M, i\neq j $$ </center>

### 二.正态分布的贝叶斯方法
贝叶斯方法最常使用的是高斯分布。原因是这种分布利于分析，且更有普遍性。
> 对于一维高斯有:
> <center> $$p(x)=\frac{1}{\sqrt{2\pi }\sigma }exp(-\frac{(x-\mu)^2}{2\sigma^2})$$ </center>
> 对于多维高斯有：
> <center> $$p(x)=\frac{1}{(2\pi)^{l/2}|\Sigma |^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma ^{-1}(x-\mu))$$ </center>
> $$\Sigma$$是$$l\times l$$的协方差矩阵, 即
> $$\Sigma=E\begin{bmatrix}\begin{bmatrix}x_1-\mu_1\\x_2-\mu_2 \end{bmatrix} &\begin{bmatrix} x_1-\mu_1, & x_2-\mu_2 \end{bmatrix} \end{bmatrix} =\begin{bmatrix} \sigma _1^2 & \sigma_{12}\\ \sigma_{12} & \sigma_2^2 \end{bmatrix}$$
> 对于二维的高斯分布，其协方差矩阵决定了分布在xOy平面的投影情况，对角矩阵投影为圆形，非对角一般投影为椭圆。
> <center> <img src="https://github.com/blogchenjl/MarkdownPic/blob/Razor_Atmel/Bayes_2.PNG?raw=true"  alt=" " /> </center>
> <center> <img src="https://github.com/blogchenjl/MarkdownPic/blob/Razor_Atmel/Bayes_3.PNG?raw=true"  alt=" " /> </center>
> 易得$$x^T\Sigma^{-1}x=\frac{x_1^2}{\sigma_1^2}+\frac{x_2^2}{\sigma_2^2}=C$$是一个椭圆方程


而贝叶斯分类器目的就是找出决策面，此时$$p(x|\omega_i)$$是每一类的分布，并且服从$$N(\mu_i, \Sigma_i)$$，用自然指数$$ln(\cdot)$$(单调增)形式和(*)有：
<center> $$g_i(x)=ln(p(x|\omega_i)P(\omega_i))=lnp(x|\omega_i)+lnP(\omega_i)=-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)+lnP(\omega_i)+c_i$$ </center>
展开可以看出这个式子是一个二次曲线，则两个二次曲线的差也为二次曲线(如椭圆，抛物线，双曲线，直线)。如果假设所有类具有相同协方差矩阵的情况下，得到更一般的形式：
<center> $$g_i(x)=w_i^Tx+w_{i0}$$ </center>
其中$$\omega_i=\Sigma^{-1}\mu_i$$和$$\omega_{i0}=lnP(\omega_i)-\frac{1}{2}\mu_i^T\Sigma_{-1}\mu_i$$，所以对于两个已知分布的高斯分布，只需求出协方差矩阵，就可得到将它们分开的超平面。

### 三.未知参数估计
在实践中，我们通常不能知道样本的概率分布，有时可能知道是哪种类型的分布，但不知道分布具体的参数(如高斯分布的均值，方差)。
#### 3.1 最大似然参数估计
$$X={x_1,x_2,\cdots,x_N}$$是从概率密度函数$$p(x;\theta)$$中采样得到，假设样本之间统计独立，则有：
<center> $$p(X;\theta)=\prod_{k=1}^{N}p(x_k;\theta)$$ </center>
最大似然法估计$$\theta$$，即计算使得似然函数最大的值($$p(x|\theta)$$最大即为$$p(x,\theta)$$最大)：
<center> $$\hat{\theta}_{ML}=argmax_\theta\prod p(x_k;\theta)$$ </center>
即对联合概率密度求导，求最大值即可，可以转换为自然对数的形式。最大似然函数有几点可取之处：
- 最大似然法是渐进一致的。对于足够大的N，最大似然估计的方差趋于零。
- 最大似然估计是渐进无偏的。即在N足够大的时候，估计的参数可认为是无偏估计。
- 最大似然估计是渐进有效的。可以达到Cramer Rao下界限制(证明略)
- 最大似然估计在N足够大的时候，概率密度函数接近于高斯分布。

#### 3.2 最大后验概率估计
在最大似然估计中，$$\theta$$是未知参数，而最大后验估计中把它当作随机向量。由贝叶斯理论有：
$$p(\theta)p(X|\theta)=p(X)p(\theta|X)或p(\theta|X)=\frac {p(\theta)p(X|\theta)} {p(X)}$$
最大后验概率(MAP)估计为使得$$p(\theta|X)$$最大的点：
<center> $$\hat{\theta}_{MAP}:\frac{\partial p(\theta|X)}{\partial \theta}=0 或 \frac{\partial p(\theta)p(X|\theta)}{\partial \theta}=0$$ </center>

#### 3.3 朴素贝叶斯分类器
对于贝叶斯分类器，需要大量的数据，且随着维度的增加，需要的数据量成指数增长。为了解决维度灾难的问题，不得不降低对准确性的要求。通常做法是假设每个特征值$$x_j，j=1,2,\cdots,l$$是统计独立的。此时：
<center> $$ p(x|\omega_i)= \prod_{j=1}^{l}p(x_j|\omega_i),i=1,2,\cdots,M $$ </center>
这样每个类的样本需求量从$$N^l$$降低为$$lN$$，这就是朴素贝叶斯分类器。分类的标准为：
<center> $$\omega_m=argmax_{\omega_i}\prod_{j=1}^{l}p(x_j|\omega_i)$$ </center>

### 四. 贝叶斯网络
朴素贝叶斯分类器虽然克服了维数灾难，但是将问题从完全依赖特征转换为相互独立特征。如果要在这两者之间找一个折衷点，那就是贝叶斯网络。由概率链式法则有：
<center> $$ p(x_1,x_2,\cdots,x_l)=p(x_l|x_{l-1},\cdots,x_1)p(x_{l-1}|x_{l-2},\cdots,x_1),\cdots,p(x_2|x_1)p(x_1) $$ </center>
而贝叶斯网则将每个特征向量$$x_i$$的条件相关性限定在特征向量子集中，即：
<center> $$p(x)=p(x_1)\prod_{i=2}^{l}p(x_i|A_i)$$ </center>
其中$$A_i \subseteq {x_{i-1},x_{i-2},\cdots,x_1}$$,
贝叶斯网络有利于研究各个特征向量之间的独立性：
- 有共同父节点的节点之间：在父节点已知的情况下相互独立；
<center> <img src="https://github.com/blogchenjl/MarkdownPic/blob/Razor_Atmel/Bayes_4.PNG?raw=true"  alt=" " /> </center>
- 在三个顺序连接的节点中：中间节点已知情况下，另外两个节点独立；
<center> <img src="https://github.com/blogchenjl/MarkdownPic/blob/Razor_Atmel/Bayes_5.PNG?raw=true"  alt=" " /> </center>
- 有共同子节点的节点之间：在子节点未知的情况下相互独立。
<center> <img src="https://github.com/blogchenjl/MarkdownPic/blob/Razor_Atmel/Bayes_6.PNG?raw=true"  alt=" " /> </center>
且每个节点的状态只与其邻居有关。
